{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data_2002_2019_for_immediate_analysis.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 0'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'Unnamed: 0.1.1.1',\n",
    "#        'Unnamed: 0.1.1.1.1', 'winner','year_kenpom', 'name_x','name_y','other_team_seed_opponent',\n",
    "#         'result',  'index', 'year_x', 'Name', 'year_y','year_adv','names_for_merging_ken',\n",
    "#         'names_for_merging_sr', 'index_everything', 'name_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# not_absolute_point_difference = []\n",
    "# for thing in df1['point_difference']:\n",
    "#     if list(df1['higher_or_lower_seed'])[i]==1:\n",
    "#         not_absolute_point_difference.append(thing)\n",
    "#         i+=1\n",
    "#     elif list(df1['higher_or_lower_seed'])[i]==0:\n",
    "#         not_absolute_point_difference.append(thing*(-1))\n",
    "#         i+=1\n",
    "#     elif list(df1['higher_or_lower_seed'])[i]==9999:\n",
    "#         not_absolute_point_difference.append(0)\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2['not_absolute_point_difference']=not_absolute_point_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2=df2.drop(['point_difference'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point_dff_with_neg = []\n",
    "# i = 0\n",
    "# for thing in df3['point_difference']:\n",
    "#     if df3['higher_or_lower_seed'][i] ==1:\n",
    "#         point_dff_with_neg.append(thing)\n",
    "#         i+=1\n",
    "#     elif df3['higher_or_lower_seed'][i] ==0:\n",
    "#         point_dff_with_neg.append(thing*(-1))\n",
    "#         i+=1\n",
    "#     elif df3['higher_or_lower_seed'][i] ==9999:\n",
    "#         point_dff_with_neg.append(0)\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3['point_dff_with_neg']=point_dff_with_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1=df1.drop(['not_absolute_point_difference'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['year','win'],axis=1)\n",
    "#could have also done answers1.set_index('SEQN') instead of dropping\n",
    "y = df['win']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "predictions = dtree.predict(X_test)\n",
    "#print(classification_report(y_test,predictions))\n",
    "\n",
    "metrics.accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "dt_cv_score = cross_val_score(dt_clf, X_train, y_train, cv=3)\n",
    "mean_dt_cv_score = np.mean(dt_cv_score)\n",
    "\n",
    "print(f\"Mean Cross Validation Score: {mean_dt_cv_score :.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6,7,8,9,10],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt_grid_search = GridSearchCV(dt_clf, dt_param_grid, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit to the data\n",
    "dt_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gs_training_score = np.mean(dt_grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "dt_gs_testing_score = dt_grid_search.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean Training Score: {dt_gs_training_score :.2%}\")\n",
    "print(f\"Mean Test Score: {dt_gs_testing_score :.2%}\")\n",
    "print(\"Best Parameter Combination Found During Grid Search:\")\n",
    "dt_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting grid search stuff back in tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy',\n",
    " max_depth=5,\n",
    " min_samples_leaf=3,\n",
    " min_samples_split=15)\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot \n",
    "features = list(X.columns)\n",
    "dot_data = StringIO()\n",
    "export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph[0].create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboots_corss_val (df,num):\n",
    "    X = df.drop(['year','win'],axis=1)\n",
    "    #could have also done answers1.set_index('SEQN') instead of dropping\n",
    "    y = df['win']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = num)\n",
    "    clf = XGBClassifier()\n",
    "    # Instantiate XGBClassifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_preds = clf.predict(X_train)\n",
    "    test_preds = clf.predict(X_test)\n",
    "    training_accuracy = accuracy_score(y_train, training_preds)\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    blah.append(test_accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_num = 525\n",
    "list_nums = []\n",
    "for thing in list(range(0,10)):\n",
    "    beginning_num+=1\n",
    "    list_nums.append(beginning_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in list_nums:\n",
    "    xgboots_corss_val(df, thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513950073421437"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(blah)/len(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate XGBClassifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training and test sets\n",
    "training_preds = clf.predict(X_train)\n",
    "test_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 93.76%\n",
      "Validation accuracy: 85.46%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of training and test sets\n",
    "training_accuracy = accuracy_score(y_train, training_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\n",
    "print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - from medium article\n",
    "https://towardsdatascience.com/a-beginners-guide-to-xgboost-87f5d4c30ed7\n",
    "Didnt use this code, used code from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train1 = pd.DataFrame(y_train)\n",
    "X_train1 = pd.DataFrame(X_train, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train1, label=y_train1)\n",
    "D_test = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM quick test - keep scrolling for cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time()\n",
    "svclassifier = SVC(kernel='rbf', C=1000) \n",
    "svclassifier.fit(X_train, y_train) \n",
    "y_pred = svclassifier.predict(X_test)\n",
    "toc = time()\n",
    "print(\"run time is {} seconds\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred)) \n",
    "print(\"The accuracy score is\" + \" \"+ str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now quick test for 2019 predictions using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2019_predict=svclassifier.predict(df_2019.drop(['year','win'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\n",
    "print(confusion_matrix(df_2019['win'],_2019_predict))  \n",
    "print(classification_report(df_2019['win'],_2019_predict)) \n",
    "print(\"The accuracy score is\" + \" \"+ str(accuracy_score(df_2019['win'], _2019_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2019=df_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2019['_2019_predict']=_2019_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2019[['team_seed','win','_2019_predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to try and cross validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "etree_glove_small_tfidf = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)),\n",
    "                                    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "                                    \n",
    "all_models = [\n",
    "    (\"glove_small_tfidf\", etree_glove_small_tfidf)]\n",
    "unsorted_scores = []\n",
    "for name, model in all_models:\n",
    "    print(f\"Running model: {name}...\")\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring=\"f1_micro\").mean()\n",
    "    print(f\"Ran model: {name}... score: {score}\")\n",
    "    unsorted_scores.append((name, score))\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", 'score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['predicts']=y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['actual']=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.to_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Import the necessary functions\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_cross_val (df, num):\n",
    "    X = df.drop(['year','win'],axis=1)\n",
    "    #could have also done answers1.set_index('SEQN') instead of dropping\n",
    "    y = df['win']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = num)\n",
    "    # Instantiate StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Transform the training and test sets\n",
    "    scaled_data_train = scaler.fit_transform(X_train)\n",
    "    scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "    # Convert into a DataFrame\n",
    "    scaled_df_train = pd.DataFrame(scaled_data_train, columns=X_test.columns)\n",
    "    scaled_df_train.head()\n",
    "    # Instantiate KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier()\n",
    "\n",
    "    # Fit the classifier\n",
    "    clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    test_preds = clf.predict(scaled_data_test)\n",
    "    blah.append(accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_num = 525\n",
    "list_nums = []\n",
    "for thing in list(range(0,10)):\n",
    "    beginning_num+=1\n",
    "    list_nums.append(beginning_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in list_nums:\n",
    "    KNN_cross_val(df, thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.696769456681351"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(blah)/len(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_difference</th>\n",
       "      <th>team_seed</th>\n",
       "      <th>higher_or_lower_seed</th>\n",
       "      <th>BLKPG</th>\n",
       "      <th>STPG</th>\n",
       "      <th>APG</th>\n",
       "      <th>AST_TO</th>\n",
       "      <th>_3pm</th>\n",
       "      <th>_3pa</th>\n",
       "      <th>_2pm</th>\n",
       "      <th>...</th>\n",
       "      <th>round</th>\n",
       "      <th>wins</th>\n",
       "      <th>losses</th>\n",
       "      <th>kenpom_adjem</th>\n",
       "      <th>wins_opponent</th>\n",
       "      <th>losses_opponent</th>\n",
       "      <th>kenpom_adjem_opponent</th>\n",
       "      <th>wins_subtracted</th>\n",
       "      <th>losses_subtracted</th>\n",
       "      <th>kenpom_adjem_subtracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.105715</td>\n",
       "      <td>1.642632</td>\n",
       "      <td>-0.116242</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.893542</td>\n",
       "      <td>-1.091370</td>\n",
       "      <td>-1.127125</td>\n",
       "      <td>-1.364415</td>\n",
       "      <td>-1.253365</td>\n",
       "      <td>-1.150182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756806</td>\n",
       "      <td>-1.899865</td>\n",
       "      <td>2.079758</td>\n",
       "      <td>-1.337261</td>\n",
       "      <td>0.585812</td>\n",
       "      <td>-0.236134</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>-1.755686</td>\n",
       "      <td>1.519800</td>\n",
       "      <td>-0.984598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151906</td>\n",
       "      <td>-0.352242</td>\n",
       "      <td>-0.115367</td>\n",
       "      <td>2.829181</td>\n",
       "      <td>-0.805155</td>\n",
       "      <td>0.410709</td>\n",
       "      <td>-0.978870</td>\n",
       "      <td>-0.630729</td>\n",
       "      <td>-0.949270</td>\n",
       "      <td>0.975724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756806</td>\n",
       "      <td>-0.783249</td>\n",
       "      <td>0.453698</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>-0.741360</td>\n",
       "      <td>0.082527</td>\n",
       "      <td>-0.133176</td>\n",
       "      <td>-0.024495</td>\n",
       "      <td>0.241960</td>\n",
       "      <td>0.118763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.086546</td>\n",
       "      <td>-1.017200</td>\n",
       "      <td>-0.115367</td>\n",
       "      <td>-1.264560</td>\n",
       "      <td>-1.350211</td>\n",
       "      <td>-0.233039</td>\n",
       "      <td>1.689722</td>\n",
       "      <td>1.203485</td>\n",
       "      <td>0.875305</td>\n",
       "      <td>-0.933772</td>\n",
       "      <td>...</td>\n",
       "      <td>1.721292</td>\n",
       "      <td>0.333367</td>\n",
       "      <td>0.128486</td>\n",
       "      <td>0.540063</td>\n",
       "      <td>0.585812</td>\n",
       "      <td>0.719849</td>\n",
       "      <td>0.456246</td>\n",
       "      <td>-0.181876</td>\n",
       "      <td>-0.396960</td>\n",
       "      <td>0.043604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.628810</td>\n",
       "      <td>-1.017200</td>\n",
       "      <td>-0.115367</td>\n",
       "      <td>-0.090447</td>\n",
       "      <td>-1.040855</td>\n",
       "      <td>0.464354</td>\n",
       "      <td>0.750773</td>\n",
       "      <td>-0.043781</td>\n",
       "      <td>0.571210</td>\n",
       "      <td>-0.284543</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895259</td>\n",
       "      <td>1.003336</td>\n",
       "      <td>-0.521937</td>\n",
       "      <td>0.801841</td>\n",
       "      <td>-0.077774</td>\n",
       "      <td>0.401188</td>\n",
       "      <td>-0.071450</td>\n",
       "      <td>0.762411</td>\n",
       "      <td>-0.609933</td>\n",
       "      <td>0.544146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.151906</td>\n",
       "      <td>1.199326</td>\n",
       "      <td>-0.116242</td>\n",
       "      <td>-1.788997</td>\n",
       "      <td>-0.458971</td>\n",
       "      <td>-1.091370</td>\n",
       "      <td>0.058916</td>\n",
       "      <td>1.056748</td>\n",
       "      <td>0.875305</td>\n",
       "      <td>-1.990360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756806</td>\n",
       "      <td>0.110043</td>\n",
       "      <td>-0.847149</td>\n",
       "      <td>-0.376161</td>\n",
       "      <td>-1.183751</td>\n",
       "      <td>0.401188</td>\n",
       "      <td>0.332794</td>\n",
       "      <td>0.919792</td>\n",
       "      <td>-0.822906</td>\n",
       "      <td>-0.446865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   point_difference  team_seed  higher_or_lower_seed     BLKPG      STPG  \\\n",
       "0          1.105715   1.642632             -0.116242 -0.020000 -0.893542   \n",
       "1          0.151906  -0.352242             -0.115367  2.829181 -0.805155   \n",
       "2         -0.086546  -1.017200             -0.115367 -1.264560 -1.350211   \n",
       "3          0.628810  -1.017200             -0.115367 -0.090447 -1.040855   \n",
       "4          0.151906   1.199326             -0.116242 -1.788997 -0.458971   \n",
       "\n",
       "        APG    AST_TO      _3pm      _3pa      _2pm  ...     round      wins  \\\n",
       "0 -1.091370 -1.127125 -1.364415 -1.253365 -1.150182  ... -0.756806 -1.899865   \n",
       "1  0.410709 -0.978870 -0.630729 -0.949270  0.975724  ... -0.756806 -0.783249   \n",
       "2 -0.233039  1.689722  1.203485  0.875305 -0.933772  ...  1.721292  0.333367   \n",
       "3  0.464354  0.750773 -0.043781  0.571210 -0.284543  ...  0.895259  1.003336   \n",
       "4 -1.091370  0.058916  1.056748  0.875305 -1.990360  ... -0.756806  0.110043   \n",
       "\n",
       "     losses  kenpom_adjem  wins_opponent  losses_opponent  \\\n",
       "0  2.079758     -1.337261       0.585812        -0.236134   \n",
       "1  0.453698      0.053903      -0.741360         0.082527   \n",
       "2  0.128486      0.540063       0.585812         0.719849   \n",
       "3 -0.521937      0.801841      -0.077774         0.401188   \n",
       "4 -0.847149     -0.376161      -1.183751         0.401188   \n",
       "\n",
       "   kenpom_adjem_opponent  wins_subtracted  losses_subtracted  \\\n",
       "0               0.239600        -1.755686           1.519800   \n",
       "1              -0.133176        -0.024495           0.241960   \n",
       "2               0.456246        -0.181876          -0.396960   \n",
       "3              -0.071450         0.762411          -0.609933   \n",
       "4               0.332794         0.919792          -0.822906   \n",
       "\n",
       "   kenpom_adjem_subtracted  \n",
       "0                -0.984598  \n",
       "1                 0.118763  \n",
       "2                 0.043604  \n",
       "3                 0.544146  \n",
       "4                -0.446865  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Transform the training and test sets\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert into a DataFrame\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns=X_test.columns)\n",
    "scaled_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "test_preds = clf.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.6871345029239766\n",
      "Recall Score: 0.6994047619047619\n",
      "Accuracy Score: 0.6945668135095447\n",
      "F1 Score: 0.6932153392330384\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "# Complete the function\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "print_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_log_reg (df, num):\n",
    "    X = df.drop(['year','win'],axis=1)\n",
    "    #could have also done answers1.set_index('SEQN') instead of dropping\n",
    "    y = df['win']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = num)\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    #score is the accuracy score\n",
    "    score = logisticRegr.score(X_test, y_test)\n",
    "    blah.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_num = 525\n",
    "list_nums = []\n",
    "for thing in list(range(0,10)):\n",
    "    beginning_num+=1\n",
    "    list_nums.append(beginning_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in list_nums:\n",
    "    cross_validation_log_reg(df, thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8443465491923641"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "sum(blah)/len(blah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function for cross validation SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val (df,num):\n",
    "    X = df.drop(['year','win'],axis=1)\n",
    "    #could have also done answers1.set_index('SEQN') instead of dropping\n",
    "    y = df['win']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = num)\n",
    "    tic = time()\n",
    "    svclassifier = SVC(kernel='rbf', C=1000) \n",
    "    svclassifier.fit(X_train, y_train) \n",
    "    y_pred = svclassifier.predict(X_test)\n",
    "    toc = time()\n",
    "    #print(\"run time is {} seconds\".format(toc-tic))\n",
    "    #print(confusion_matrix(y_test,y_pred))  \n",
    "    #print(classification_report(y_test,y_pred)) \n",
    "    #print(\"The accuracy score is\" + \" \"+ str(accuracy_score(y_test, y_pred)))\n",
    "    blah.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_num = 525\n",
    "list_nums = []\n",
    "for thing in list(range(0,10)):\n",
    "    beginning_num+=1\n",
    "    list_nums.append(beginning_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in list_nums:\n",
    "    cross_val (df,thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8619676945668135"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "sum(blah)/len(blah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
